{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements needed\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torchvision\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU if applicable\n",
    "print(torch.cuda.is_available())\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset + Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download Caltech101 Data, Only need to run this cell once, comment this out after using it once\n",
    "dataset = datasets.Caltech101(\n",
    "    root=\"dataset\",\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shouldn't need to change but sometimes os starts being wierd absolute path maybe needed\n",
    "path_to_data = 'dataset\\caltech101\\\\101_ObjectCategories'\n",
    "\n",
    "# We are using an ImageFolder method since for some reason I had issues extracting the classes using the dataset.Caltech101 object\n",
    "dataset = datasets.ImageFolder(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter the dataset and keep only the top k classes + return fresh model for it\n",
    "# dataset : Full dataset\n",
    "# top_k   : How many of the top k-categories do you want to use\n",
    "# image   : Desired image size for dataset\n",
    "# seed    : Generation seed default = 42\n",
    "# test_split: amount of data split into test/train set default = .8\n",
    "# percent_data : % percentage of the data to include (useful for trimming dataset) default = 1.00 (full dataset)\n",
    "# create_k1 : Boolean of if to return a K+1 dataset (only for Outlier Test Scenario)\n",
    "def create_subset(dataset, top_k, image_size, seed = 42, test_split = .8, percent_data = 1.00, create_k1 = False):\n",
    "    assert test_split >= 0.0 and test_split <= 1.0\n",
    "\n",
    "    # Apply transforms to fit the desired sizes \n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "    dataset.transform = transform\n",
    "\n",
    "    # find the highest occuring image categories (so we can have more data for training)\n",
    "    count = {}\n",
    "    for _, label in dataset:\n",
    "        if label in count.keys():\n",
    "            count[label] += 1\n",
    "        else:\n",
    "            count[label] = 1\n",
    "    # Sort in descending order as a list of tuples: [(labels, count), ...]\n",
    "    sorted_labels = sorted(count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top_k class into a list\n",
    "    top_k_class = [class_label for class_label, _ in sorted_labels[:top_k]]\n",
    "\n",
    "    # Convert to (name, label) tuple for future mapping (if needed)\n",
    "    # NOTE: This resets the indexing to be ranging from (0 to k-1) ex. for k = 2 labels will be 0 and 1 (2 classes)\n",
    "    class_mapping = [(dataset.classes[x], i) for i, x in enumerate(top_k_class)]\n",
    "\n",
    "    subset = []\n",
    "    # Create a new dataset with samples from the selected classes\n",
    "    for i, (data, target) in enumerate(dataset):\n",
    "        if target in top_k_class:\n",
    "            subset.append((data, top_k_class.index(target)))\n",
    "    \n",
    "    # Make number of channels consistant (some pics are B&W so we will convert them to 3 channels by duplicating the single channel)\n",
    "    new_dataset = []\n",
    "    for i in range(len(subset)):\n",
    "        if subset[i][0].shape != torch.Size([3, image_size, image_size]):\n",
    "            new_dataset.append((subset[i][0].repeat(3,1,1) , subset[i][1]))\n",
    "        else:\n",
    "            new_dataset.append((subset[i][0] , subset[i][1]))\n",
    "\n",
    "    # For K+1\n",
    "    if create_k1:\n",
    "        k_data = new_dataset.copy()\n",
    "        unknown_images = []\n",
    "        for data, target in dataset:\n",
    "            if target not in top_k_class:\n",
    "                unknown_images.append((data, top_k))\n",
    "        random.shuffle(unknown_images)\n",
    "        k_data.extend(unknown_images[:len(new_dataset)])\n",
    "\n",
    "    # Fix the seed for replication\n",
    "    seeded = torch.Generator().manual_seed(seed)\n",
    "    x = [test_split * percent_data, (1 - test_split) * percent_data, 1-percent_data]\n",
    "    train, test, _ = torch.utils.data.random_split(new_dataset, x, generator = seeded)\n",
    "\n",
    "    # Return a fresh model to train on\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained = True)\n",
    "\n",
    "    # EfficientNet was frankly too good it was converging to 100% accuracy rapidly and would make for poor comparisons \n",
    "    #model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=top_k)\n",
    "    if create_k1:\n",
    "        seeded = torch.Generator().manual_seed(seed)\n",
    "        x = [test_split * percent_data, (1 - test_split) * percent_data, 1-percent_data]\n",
    "        train_k, test_k, _ = torch.utils.data.random_split(k_data, x, generator = seeded)\n",
    "        return train, test, train_k, test_k, class_mapping, model\n",
    "\n",
    "    return train, test, class_mapping, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_loop(loss_func, optimizer, epochs, scheduler, train_dataloader, test_dataloader, model):\n",
    "    # variables needed for metrics later\n",
    "    train_losses = []\n",
    "    test_losses = 0\n",
    "    train_accuracy = []\n",
    "    test_accuracy = 0\n",
    "    start_time_train = time.time()\n",
    "    ############################ Train Loop ############################\n",
    "    for i in range(epochs):\n",
    "        # variables needed for metrics later\n",
    "        train_size = len(train_dataloader.dataset)\n",
    "        # makes sure to set model to train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_num_batches = len(train_dataloader)\n",
    "        for batch, (X, labels) in enumerate(train_dataloader):\n",
    "            # Make sure values are on correct device\n",
    "            X = X.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Model pred + loss\n",
    "            pred = model(X)\n",
    "            loss = loss_func(pred, labels)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute metrics\n",
    "            train_loss+=loss.item()\n",
    "            train_correct+=(pred.argmax(axis = 1) == labels).type(torch.float).sum().item()\n",
    "        # Compute metrics\n",
    "        train_losses.append(train_loss/train_num_batches)\n",
    "        train_accuracy.append(train_correct/train_size)\n",
    "        # Update scheduler \n",
    "        scheduler.step()\n",
    "\n",
    "    end_time_train = time.time()\n",
    "    train_time = end_time_train - start_time_train\n",
    "    ############################ Train Loop ############################\n",
    "    \n",
    "    ############################ Test Loop #############################\n",
    "    test_size = len(test_dataloader.dataset)\n",
    "    test_num_batches = len(test_dataloader)\n",
    "    # makes sure to set model to eval\n",
    "    model.eval()\n",
    "    # variables needed for metrics later\n",
    "    start_time_test = time.time()\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, labels in test_dataloader:\n",
    "            # Make sure values are on correct device\n",
    "            X = X.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Model pred + loss\n",
    "            pred = model(X)\n",
    "            loss = loss_func(pred, labels)\n",
    "\n",
    "            # Compute metrics\n",
    "            test_loss+=loss.item()\n",
    "            test_correct+=(pred.argmax(axis = 1) == labels).type(torch.float).sum().item()\n",
    "        # Compute metrics\n",
    "        test_losses = test_loss/test_num_batches\n",
    "        test_accuracy = test_correct/test_size\n",
    "    \n",
    "    end_time_test = time.time()\n",
    "    test_time = end_time_test - start_time_test\n",
    "    ############################ Test Loop #############################\n",
    "\n",
    "    return train_accuracy, train_losses, test_accuracy, test_losses, train_time, test_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training/Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact high-level loop that runs everything and allows modification of all variables + make pd dataframe to store all information\n",
    "graph_data = []\n",
    "columns = [\"Size\", \"K\", \"Device\", \"Train_Time\", \"Test_Time\", \"Train_Acc\", \"Test_Acc\", \"Percent_data_used\", \"Training_Samples\", \"Testing_Samples\"]\n",
    "df = pd.DataFrame(columns = columns)\n",
    "for percent in [1]:\n",
    "    for size in [64, 128]:\n",
    "        for k in [10, 50]:\n",
    "            for device in ['cuda', 'cpu']:\n",
    "                train, test, class_mapping, model = create_subset(dataset, k, size, percent_data = percent)\n",
    "                model.to(device)\n",
    "                ############################################# HYPER PARAMS #############################################\n",
    "                batch_size = 16\n",
    "                loss_func = nn.CrossEntropyLoss()\n",
    "                lr = .001\n",
    "                weight_decay = .0001\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "                epochs = 20\n",
    "                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=epochs/3, gamma=.5) # Set gamma < 1 to add effects of a scheduler and set gamma = to negate the \n",
    "                ############################################# HYPER PARAMS #############################################\n",
    "                train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size)\n",
    "                test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size)\n",
    "                train_accuracy, train_losses, test_accuracy, test_losses, train_time, test_time = train_test_loop(loss_func, optimizer, epochs, scheduler, train_dataloader, test_dataloader, model)\n",
    "                # Add results to table\n",
    "                data = [size, k, device, train_time, test_time, train_accuracy[-1], test_accuracy, percent, len(train), len(test)]\n",
    "                row = pd.DataFrame([data], columns=columns)\n",
    "                df = pd.concat([df, row], ignore_index=True)\n",
    "                print(data)\n",
    "                graph_data.append((data, train_accuracy, train_losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph to look at accuracy curves to experiment with proper epochs needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, train_accuracy, train_losses in graph_data:\n",
    "    x = np.arange(0, epochs)\n",
    "    plt.title(\"Train Accuracy Curve\")\n",
    "    plt.xlabel(\"Accuracy\")\n",
    "    plt.ylabel(\"Epochs\")\n",
    "    plt.plot(x, train_accuracy, color =\"red\", label = \"Train Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TAKEN FROM ChatGPT (credit to them for this block of code)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained = False)\n",
    "def get_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    # Assuming each parameter is a 32-bit float (4 bytes)\n",
    "    total_size = total_params * 4 / (1024 ** 2)  # Convert to megabytes (MB)\n",
    "    return total_size\n",
    "x = get_model_size(model)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through different values of size and k\n",
    "for size in [64, 128]:\n",
    "    for k in [10, 50]:\n",
    "        print(f\"Size: {size}, K: {k}\")\n",
    "        # Create the subset\n",
    "        train, test, class_mapping, model = create_subset(dataset, k, size)\n",
    "        \n",
    "        # number of categories\n",
    "        num_categories = len(class_mapping)\n",
    "        \n",
    "        # 5 images per column\n",
    "        fig_width = 5 * num_categories * 2\n",
    "        fig_height = num_categories * 2\n",
    "        \n",
    "        # Generate and display 5 random images per category as one row\n",
    "        for class_name, label in class_mapping:\n",
    "            category_images = [img for img, l in train if l == label]\n",
    "            random_indices = random.sample(range(len(category_images)), 5)\n",
    "            \n",
    "            # Adjust figure size for displaying images\n",
    "            plt.figure(figsize=(fig_width, fig_height))\n",
    "            \n",
    "            for i, idx in enumerate(random_indices):\n",
    "                img = category_images[idx]\n",
    "                # expects H x W x C images not C x H x W\n",
    "                img = np.transpose(img, (1, 2, 0)) \n",
    "                plt.subplot(1, num_categories * 5, i + 1)\n",
    "                plt.imshow(img)\n",
    "                if i == 0:  # only for first image prevents\n",
    "                    plt.title(class_name)\n",
    "                plt.axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K + 1 test, Outlier Test Scenario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "columns = [\"Size\", \"K\", \"Train_Time\", \"Test_Time\", \"Train_Acc\", \"Test_Acc\", \"Training_Samples\", \"Testing_Samples\"]\n",
    "df = pd.DataFrame(columns = columns)\n",
    "k_test = False\n",
    "for k in [2, 2, 5, 5, 10, 10, 15, 15]:\n",
    "    if k_test:\n",
    "        k += 1\n",
    "        k_test = False\n",
    "        _, _, train, test, class_mapping, model = create_subset(dataset, k, 64, create_k1= True)\n",
    "    else:\n",
    "        k_test = True\n",
    "        train, test, class_mapping, model = create_subset(dataset, k, 64)\n",
    "    model.to(device)\n",
    "    ############################################# HYPER PARAMS #############################################\n",
    "    batch_size = 16\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    lr = .001\n",
    "    weight_decay = .0001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    epochs = 10\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=epochs/3, gamma=.5) # Set gamma < 1 to add effects of a scheduler and set gamma = to negate the \n",
    "    ############################################# HYPER PARAMS #############################################\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size)\n",
    "    train_accuracy, train_losses, test_accuracy, test_losses, train_time, test_time = train_test_loop(loss_func, optimizer, epochs, scheduler, train_dataloader, test_dataloader, model)\n",
    "    # Add results to table\n",
    "    data = [size, k, train_time, test_time, train_accuracy[-1], test_accuracy, len(train), len(test)]\n",
    "    row = pd.DataFrame([data], columns=columns)\n",
    "    df = pd.concat([df, row], ignore_index=True)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results_k.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
